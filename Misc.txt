// Notes on all other technical stuff I learn.

@@@@@@@@@@@@@@@@@@@@@@@@@     TOR    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Tor:
- Passing traffic by bouncing them across various Tor nodes across the globe.
  Minimum, Tor traffic passes through 3 nodes: Entry/Gaurd, Relay & Exit.

- Traffic is encrypted in layers (like Onion) before it is sent out from source.
    # of layers of encryption = # of Tor nodes traffic is passing.

- As traffic passes through each node, that node knows how to decrypt it's layer
  and forward to next Tor node.


@@@@@@@@    MUNICIPAL SOLID WASTE MANAGEMENT IN DEVELOPING COUNTRIES @@@@@@@@

Facts:
- Humans generate 7-10 billion tons of waste per year worldwide.
- Waste from 2 billion people is not collected from their homes and
  neighborhoods.
- By end of century, 83% of world population will be in Asia and Africa.
- There are 3 billion people who lack access to controlled disposal facilities.
- By 2030 there will be 40 megacities (cities with 10 million or more
  population). Of that 32 will be in Asia and Africa.


Municipal Waste Generation and Characterization:
- Estimating Lower Calorific Value (LCV)
    LCV = 40(a+b+c+d) + 90e - 46W
    LCV in kcal/kg
    a = Paper
    b = Textiles
    c = Wood and leaves
    d = Food waste
    e = Plastic and rubber
    W = Water
    all of above in % of wet weight.

- Incineration without adding fuel requies LCV > 1000 kcal/kg
- Incineration with energy recovery requires LCV > 1500-1650 kcal/kg


[1]
https://www.coursera.org/learn/solid-waste-management/home/welcome

@@@@@@@@@@@@@   Open/R  @@@@@@@@@@@@@@

- Build a routing _platform_ and not a routing _protocol_
- Applications are built on top of this platform in C++. Routing is one
  application.
- Each node in this network as following 4 components:
    * Decision
    * FIB
    * KV-Store
    * Link Monitor
- Zero MQ for message passing between nodes.
- Thrift: Message encoding. Populat open source library.

- Key-Value Store:
    - Key = Strings, Value = Thrift objects

Optimization in KV Store:
- Group nodes in to clusters
- STP within clusters
- Between clusters, similar to BGP's path tracing mechanism is used.

Scaling in KV Store:
-

Neighbor Discovery:
- Runs as a separate process.
- Works on link-level




[1]
https://code.facebook.com/posts/1036362693099725/networking-scale-may-2016-recap/

[2]
https://code.facebook.com/posts/1142111519143652/introducing-open-r-a-new-modular-routing-platform/


@@@@@@@@@@@@@   Networks Illustrated: Principles w/o Calculus @@@@@@@@@@@@@@

Lesson 1:
---
- 8 principles in this course:
- Sharing (network medium) is hard
- Consensus is hard
    * Referral system.
    * Auction system with systems bidding.
    * Voting system.
- Crowds are wise
    * Amazon trying to solve Rating problem.
    * Netflix trying to solve Recommendation problem.

- Crowds are NOT wise:
    * Information cascade can happen and crowds may not think for themselves.

- Network is Expensive:
- Network of Networks:
    * internet is combination of many networks within networks.

- Layers on Layers:
- Bigger & Bigger:


Lesson 2:
---
- Mobile penetration rate:
    # of mobile subscriptions/total population

- Multiple Access
    * Morse devised Morse Code to use Telegraph Cable to transmit ./- notation.
    * Bell devised multiple transmitters and recievers on same Telegraph Cable.

- Frequency Division Multiple Access (FDMA)
    * N MHz = N cycles per second.

- 0G
    MTS (1946) - Mobile Telephone System
        * FDMA system
        * An operator was required
    IMTS (1964) - Improved MTS
        * No operator required
    DynaTAC (1973)
        * From Qualcomm

- Attenuation
    WiFi is in unlicensed frequency
    Cellular is in licensed frequency
    Attenuation is decaying of signal over distance

- Cell & 1G
    + Due to attenuation, we divided regions in to cells of hexagon shape.
    + Cell towers are at the center of hexagons and base stations at
      intersection of edges of hexagons. With this design, a base station can
      serve 3 cells.
    + Frequency ranges in one cell doesn't match with next cell to avoid
      interference, but they can match with non-adjacent cells. This is done for
      spectral efficiency.

- Frequency Reuse Factor
    + Number of frequency bands used.

- 1G
    + Cells were used first time in 1G.
    + AMPS (1986) - Advanced Mobile Phone System. Operated in 800 MHz band
      system.
    + Cellular technology transformed from 0G to 1G.
    + First operated in Chicago with 90 people and 10 cells. Demonstrated
      feasibility.

- 2G
    + Migration from Analog to Digital is the most significant change from 1G to
      2G
    + Digital screens started appearing.
    + Texting was a feature.
    + 2000 was the year when 2G came.

- TDMA (Time Division)
    + TDMA uses Frequency Division with Time Division.
    + Why didn't we do this before? Analog signalling didn't allow this.
    + Sometimes called F/TDMA.

- GSM (Global System for Mobile Communication) 1982
    + First system to employ TDMA
    + 3x more than Analog capacity.
    + GSM was quickly adopted by Europe.

- CDMA (Code Division Multiple Access) 1988
    + CTIA (Cellular Technology Industries Assoc) wanted 10x improvement.
    + Qualcomm had a different idea. They added a different dimension to time
      and frequency. A code is added with each communication.

- Codes?
    + Along with digital data signal, a "spreading code" is used to transmit
      data. data signal x spreading code is transmitted.
    + Receiver uses spreading code to decode data signal.
    + Designing spreading code is very tough.
    + It needs to have a property called "Orthogonal" that two spreading codes
      should cancel out.

- Qualcomm's CDMA improvements
    + Supposedly 40x improvement (theoretically). But in reality, much less.
    + CTIA voted Qualcomm's CDMA as a 2G standard in US.

- Near-Far Problem
    + Channel quality depends on distance from base station and obstacles in
      between.
    + How much power is needed to transmit data? It's measure in Watts (W). In
      fact, phones need power in milli Watts (mW) to transmit data.
    + Phones that are farther from BS transmit data at, say, 20 mW and by the
      time it reaches BS, it's say 2 mW. While those near to BS may be received
      at 10 mW. It is important to make these signals reach BS at constant
      power. There's easy algo for this.
    + BS tells phones at what power to transmit so it receives at constant
      power.
    + Transmit Power Control (TCP) Algorithm to solve NF Problem.

- Signal Quality
    + Power is not the problem. Quality is.
    + Increasing power by a phone causes interference.
    + Signal to Interference Ratio (SIR).

- DPC (Distributed Power Control)
    + Cells transmit data to BS. BS sends SIR to phones. Phones adjust power.
    + This process iterates and finally converges on a power.




[1] https://www.coursera.org/learn/networks-illustrated/home



@@@@@@@@@@@@@   Cloud Computing Concepts Part 1    @@@@@@@@@@@@@@

- AWS
    * EC2: Elastic Compute Cloud
    * S3: Simple Storage Service
    * EBS: Elastic Block Storage

- 4 Features New in Today's Cloud
    * Massive Scale
    * On-Demand Access: Pay as you go model.
    * Data intensive nature
    * New cloud programming paradigms

- Energy usage:
    WUE (Water Usage Efficiency) = Annual Water Usage/IT Equipment Energy
    (L/kWh) - low is good.
    PUE (Power Usage Efficiency) = Total Facility Power/IT Equipment Power

- MapReduce:
    * Map
    * Reduce
    * Resource Manager (for Hadoop, YARN is the resource mgr)

- YARN Scheduler = Yet Another Resource Negotiator
    + Treats each server as collection of 'containers'
        ~ Container = CPU + Memory (not same as Docker container)
    + 3 main components
        ~ Global Resource Manager (RM) for scheduling
        ~ Per-server Node Manager (NM) for server specific functions
        ~ Per-application Application Manager (AM)
- MapReduce Fault Tolerance
    + Heartbeats are periodically sent across managers for health checks.
    + Stragglers: Slow execution.
        ~ Speculative Execution: If % executed is very low, a replicate
          execution is started on a new VM/container.

- Building blocks of Distributed Systems
    + Gossip and Epidemic Protocols
    + Failure Detection and Membership Protocols
- Grid computing is pre-cursor to cloud computing.


Multicast Problem
---
- Multicast protocol must be fault-tolerant and scalable.
- IP multicast is not attractive because it is MAY not be implemented in
  underlying routers/servers.
- This is application level multicast protocol.

How to solve?
---
Centralized: Sender has a list of destination node. Opens socket and sends
             TCP/UDP packets.
- Problem?
    * fault-tolerance: It is not.
    * more latency (longer time)

Tree based multicast protocols:
    + eg., IP multicast, SRM, RMTP, TRAM, TMTP
- Problems? If nodes fail, re-create the tree.
- Spanning trees are built among nodes. This is used to disseminate messages.
- Use ACKs or NACKs to repair unreceived multicast packets.

SRM (Scalable Reliable Multicast):
    + Uses NACKs
    + Uses random delays and exponential backoff to avoid NACK storms

RMTP (Reliable Multicast Transport Protocol):
    + Uses ACKs
    + ACKs sent to designated receivers, which then re-transmit missing
      multicasts.

- These protocols still cause O(N) ACK/NACK overhead.

Gossip Protocol (Epidemic Multicast)
---
- Sender randomly picks 'b' random targets and sends Gossip Messages.
- 'b' is called Gossip fan out. Typically 2 or so targets.
- When nodes receive the Gossip, it is 'infected' by Gossip.
- Receiver starts doing the same.
- There may be duplicate received by nodes.
- "Infected Nodes" - those that received multicast message.
- "Uninfected Nodes" - those that did NOT receive multicast message.

Push vs Pull
---
- Gossip protocol is a push protocol.
- If multiple multicast messages exist, gossip a subset of them.
- There's a "Pull" gossip.
    + Periodically poll a few randomly selected targets for new multicast
      messages that you haven't received.
- There's a hybrid variant too: push-pull.

Gossip Analysis
---
- Push based Gossip protocol is
    + lightweight in large groups.
    + spreads multicast quickly.
    + is highly fault-tolerant.

- Analysis is based on mathematical branch of Epidemiology.
    + Population of n+1 individuals mixing homogeneously.
    + Contact rate between any individual pair is B

- TODO: Watch videos for analysis.

Group Membership
---
- Failure rate are norm in datacenters.

- Say, the rate of failure of one machine is 10 years (120 months). In a DC with
  120 servers, Mean Time To Failure (MTTF) is 1 month. MTTF reduces with more
  servers.

- Two Sub Protocols are essential in Group Membership.
    + Failure Detection
    + Info Dissemination

Failure Detectors
---
- Frequency of failures goes up linearly with size of DC.
- Properties of Failure Detectors
    + Completeness - each failure is detected.
    + Accuracy - no mistaken detection.
    + Speed - time to first detection of failure.
    + Scale - load balance each member.

- Completeness + Accuracy = Very hard to acheive. Consensus problem in
  distributed systems.

- What *really* happens?
    + Completeness - Gauranteed
    + Accuracy - partial/probabilistic gaurantee.


Failure Detector Protocols

Centralized Heartbeating.
---
- A process receives heartbeat from all other processes. After timeout, a
  process is marked failed.
- A process can be overloaded with heartbeats. Hotspot problemm.

Ring Heartbeating
---
- Form a ring. Processes send/receive heartbeats to its neighbors.
- Multiple simultaneous failures causes problems.

All-to-All Heartbeating
---
- All processes send to all other processes.
- Has equal load per member.
- Protocol is complete.
- Too many messages though.


Gossip-Style Membership
---
- Nodes maintain a membership list with 3 values:
    + IDs of Nodes.
    + Heartbeat counter. A counter unique to a node.
    + Local time when heartbeat was received from a node.

- Each node randomly shares this membership list with peers. Peers can update
  their membership list based on this message.

- There's a timeout if heartbeat is not received from a node, called T_fail.
- After a wait time of T_cleanup (mostly same as T_fail), the entry is deleted.

- If a heartbeat is received *directly* from the node then entry is added back
  to membership table.

- If membership tables are received from other nodes in T_fail time and before
  T_cleanup time, not updated.


Which is best failure detector?
---
- Failure Detector Properties:
    + Completeness - Guarantee always
    + Accuracy - Probability Mistake in T time units: PM(T)
    + Speed - T time units (time to first failure detection)
    + Scale - N x L (nodes x load per node)

- All-to-All heartbeating:
    + Each node sends N heartbeats every T time units. Load = N/T

- Gossip-Style:
    + tg = gossip period.
    + T = time when all nodes receive gossip.
    + T = (log N) x tg
    + L = Load per node = N/tg = (N x (log N))/T

- What's best/optimal?
    + TODO: Watch video 2.4 of Week 2 for analysis


SWIM Failure Detection Protocol
---
- TODO: Watch videos 2.5 & 2.6 of Week 2 for analysis.


Grid Computing
---
- Scheduling jobs on the Grid is one problem.
- Globus Protocol - inter-site job scheduling protocol
- HTCondor Protocol (High Through Condor) - intra-site job scheduling and
  monitoring protocol.
- Globus Alliance - bunch of univs and research labs.
- Globus Toolkit: Lot of tools (free and open-source) for Grid computing.

- Is Grid Computing and Cloud Computing converging? It's an open question.


@@@@@@@@@@@@@@@@@@@@@@@   C++ for C Programmers    @@@@@@@@@@@@@@@@@@@@@@@@

- static_cast <new_type> (expression), dynamic_cast <new_type> (expression)
  const_cast <new_type> (exp), reinterpret_cast <new_type> (exp)

C++ is better than C (really?)
---
    + More type safe
    + More libraries
    + Less reliance on preprocessor
    + OO vs imperative

C++ Templates
---
    template <class T>
    inline void swap (T& d, T& s)
    {   T temp = s;
        s = d;
        d = temp;
    }
