// Notes on all other technical stuff I learn.

@@@@@@@@@@@@@@@@@@@@@@@@@     TOR    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Tor:
- Passing traffic by bouncing them across various Tor nodes across the globe.
  Minimum, Tor traffic passes through 3 nodes: Entry/Gaurd, Relay & Exit.

- Traffic is encrypted in layers (like Onion) before it is sent out from source.
    # of layers of encryption = # of Tor nodes traffic is passing.

- As traffic passes through each node, that node knows how to decrypt it's layer
  and forward to next Tor node.


@@@@@@@@    MUNICIPAL SOLID WASTE MANAGEMENT IN DEVELOPING COUNTRIES @@@@@@@@

Facts:
- Humans generate 7-10 billion tons of waste per year worldwide.
- Waste from 2 billion people is not collected from their homes and
  neighborhoods.
- By end of century, 83% of world population will be in Asia and Africa.
- There are 3 billion people who lack access to controlled disposal facilities.
- By 2030 there will be 40 megacities (cities with 10 million or more
  population). Of that 32 will be in Asia and Africa.


Municipal Waste Generation and Characterization:
- Estimating Lower Calorific Value (LCV)
    LCV = 40(a+b+c+d) + 90e - 46W
    LCV in kcal/kg
    a = Paper
    b = Textiles
    c = Wood and leaves
    d = Food waste
    e = Plastic and rubber
    W = Water
    all of above in % of wet weight.

- Incineration without adding fuel requies LCV > 1000 kcal/kg
- Incineration with energy recovery requires LCV > 1500-1650 kcal/kg


[1]
https://www.coursera.org/learn/solid-waste-management/home/welcome

@@@@@@@@@@@@@   Open/R  @@@@@@@@@@@@@@

- Build a routing _platform_ and not a routing _protocol_
- Applications are built on top of this platform in C++. Routing is one
  application.
- Each node in this network as following 4 components:
    * Decision
    * FIB
    * KV-Store
    * Link Monitor
- Zero MQ for message passing between nodes.
- Thrift: Message encoding. Populat open source library.

- Key-Value Store:
    - Key = Strings, Value = Thrift objects

Optimization in KV Store:
- Group nodes in to clusters
- STP within clusters
- Between clusters, similar to BGP's path tracing mechanism is used.

Scaling in KV Store:
-

Neighbor Discovery:
- Runs as a separate process.
- Works on link-level




[1]
https://code.facebook.com/posts/1036362693099725/networking-scale-may-2016-recap/

[2]
https://code.facebook.com/posts/1142111519143652/introducing-open-r-a-new-modular-routing-platform/


@@@@@@@@@@@@@   Networks Illustrated: Principles w/o Calculus @@@@@@@@@@@@@@

Lesson 1:
---
- 8 principles in this course:
- Sharing (network medium) is hard
- Consensus is hard
    * Referral system.
    * Auction system with systems bidding.
    * Voting system.
- Crowds are wise
    * Amazon trying to solve Rating problem.
    * Netflix trying to solve Recommendation problem.

- Crowds are NOT wise:
    * Information cascade can happen and crowds may not think for themselves.

- Network is Expensive:
- Network of Networks:
    * internet is combination of many networks within networks.

- Layers on Layers:
- Bigger & Bigger:


Lesson 2:
---
- Mobile penetration rate:
    # of mobile subscriptions/total population

- Multiple Access
    * Morse devised Morse Code to use Telegraph Cable to transmit ./- notation.
    * Bell devised multiple transmitters and recievers on same Telegraph Cable.

- Frequency Division Multiple Access (FDMA)
    * N MHz = N cycles per second.

- 0G
    MTS (1946) - Mobile Telephone System
        * FDMA system
        * An operator was required
    IMTS (1964) - Improved MTS
        * No operator required
    DynaTAC (1973)
        * From Qualcomm

- Attenuation
    WiFi is in unlicensed frequency
    Cellular is in licensed frequency
    Attenuation is decaying of signal over distance

- Cell & 1G
    + Due to attenuation, we divided regions in to cells of hexagon shape.
    + Cell towers are at the center of hexagons and base stations at
      intersection of edges of hexagons. With this design, a base station can
      serve 3 cells.
    + Frequency ranges in one cell doesn't match with next cell to avoid
      interference, but they can match with non-adjacent cells. This is done for
      spectral efficiency.

- Frequency Reuse Factor
    + Number of frequency bands used.

- 1G
    + Cells were used first time in 1G.
    + AMPS (1986) - Advanced Mobile Phone System. Operated in 800 MHz band
      system.
    + Cellular technology transformed from 0G to 1G.
    + First operated in Chicago with 90 people and 10 cells. Demonstrated
      feasibility.

- 2G
    + Migration from Analog to Digital is the most significant change from 1G to
      2G
    + Digital screens started appearing.
    + Texting was a feature.
    + 2000 was the year when 2G came.

- TDMA (Time Division)
    + TDMA uses Frequency Division with Time Division.
    + Why didn't we do this before? Analog signalling didn't allow this.
    + Sometimes called F/TDMA.

- GSM (Global System for Mobile Communication) 1982
    + First system to employ TDMA
    + 3x more than Analog capacity.
    + GSM was quickly adopted by Europe.

- CDMA (Code Division Multiple Access) 1988
    + CTIA (Cellular Technology Industries Assoc) wanted 10x improvement.
    + Qualcomm had a different idea. They added a different dimension to time
      and frequency. A code is added with each communication.

- Codes?
    + Along with digital data signal, a "spreading code" is used to transmit
      data. data signal x spreading code is transmitted.
    + Receiver uses spreading code to decode data signal.
    + Designing spreading code is very tough.
    + It needs to have a property called "Orthogonal" that two spreading codes
      should cancel out.

- Qualcomm's CDMA improvements
    + Supposedly 40x improvement (theoretically). But in reality, much less.
    + CTIA voted Qualcomm's CDMA as a 2G standard in US.

- Near-Far Problem
    + Channel quality depends on distance from base station and obstacles in
      between.
    + How much power is needed to transmit data? It's measure in Watts (W). In
      fact, phones need power in milli Watts (mW) to transmit data.
    + Phones that are farther from BS transmit data at, say, 20 mW and by the
      time it reaches BS, it's say 2 mW. While those near to BS may be received
      at 10 mW. It is important to make these signals reach BS at constant
      power. There's easy algo for this.
    + BS tells phones at what power to transmit so it receives at constant
      power.
    + Transmit Power Control (TCP) Algorithm to solve NF Problem.

- Signal Quality
    + Power is not the problem. Quality is.
    + Increasing power by a phone causes interference.
    + Signal to Interference Ratio (SIR).

- DPC (Distributed Power Control)
    + Cells transmit data to BS. BS sends SIR to phones. Phones adjust power.
    + This process iterates and finally converges on a power.




[1] https://www.coursera.org/learn/networks-illustrated/home



@@@@@@@@@@@@@   Cloud Computing Concepts Part 1    @@@@@@@@@@@@@@

- AWS
    * EC2: Elastic Compute Cloud
    * S3: Simple Storage Service
    * EBS: Elastic Block Storage

- 4 Features New in Today's Cloud
    * Massive Scale
    * On-Demand Access: Pay as you go model.
    * Data intensive nature
    * New cloud programming paradigms

- Energy usage:
    WUE (Water Usage Efficiency) = Annual Water Usage/IT Equipment Energy
    (L/kWh) - low is good.
    PUE (Power Usage Efficiency) = Total Facility Power/IT Equipment Power

- MapReduce:
    * Map
    * Reduce
    * Resource Manager (for Hadoop, YARN is the resource mgr)

- YARN Scheduler = Yet Another Resource Negotiator
    + Treats each server as collection of 'containers'
        ~ Container = CPU + Memory (not same as Docker container)
    + 3 main components
        ~ Global Resource Manager (RM) for scheduling
        ~ Per-server Node Manager (NM) for server specific functions
        ~ Per-application Application Manager (AM)
- MapReduce Fault Tolerance
    + Heartbeats are periodically sent across managers for health checks.
    + Stragglers: Slow execution.
        ~ Speculative Execution: If % executed is very low, a replicate
          execution is started on a new VM/container.

- Building blocks of Distributed Systems
    + Gossip and Epidemic Protocols
    + Failure Detection and Membership Protocols
- Grid computing is pre-cursor to cloud computing.


Multicast Problem
---
- Multicast protocol must be fault-tolerant and scalable.
- IP multicast is not attractive because it is MAY not be implemented in
  underlying routers/servers.
- This is application level multicast protocol.

How to solve?
---
Centralized: Sender has a list of destination node. Opens socket and sends
             TCP/UDP packets.
- Problem?
    * fault-tolerance: It is not.
    * more latency (longer time)

Tree based multicast protocols:
    + eg., IP multicast, SRM, RMTP, TRAM, TMTP
- Problems? If nodes fail, re-create the tree.
- Spanning trees are built among nodes. This is used to disseminate messages.
- Use ACKs or NACKs to repair unreceived multicast packets.

SRM (Scalable Reliable Multicast):
    + Uses NACKs
    + Uses random delays and exponential backoff to avoid NACK storms

RMTP (Reliable Multicast Transport Protocol):
    + Uses ACKs
    + ACKs sent to designated receivers, which then re-transmit missing
      multicasts.

- These protocols still cause O(N) ACK/NACK overhead.

Gossip Protocol (Epidemic Multicast)
---
- Sender randomly picks 'b' random targets and sends Gossip Messages.
- 'b' is called Gossip fan out. Typically 2 or so targets.
- When nodes receive the Gossip, it is 'infected' by Gossip.
- Receiver starts doing the same.
- There may be duplicate received by nodes.
- "Infected Nodes" - those that received multicast message.
- "Uninfected Nodes" - those that did NOT receive multicast message.

Push vs Pull
---
- Gossip protocol is a push protocol.
- If multiple multicast messages exist, gossip a subset of them.
- There's a "Pull" gossip.
    + Periodically poll a few randomly selected targets for new multicast
      messages that you haven't received.
- There's a hybrid variant too: push-pull.

Gossip Analysis
---
- Push based Gossip protocol is
    + lightweight in large groups.
    + spreads multicast quickly.
    + is highly fault-tolerant.

- Analysis is based on mathematical branch of Epidemiology.
    + Population of n+1 individuals mixing homogeneously.
    + Contact rate between any individual pair is B

- TODO: Watch videos for analysis.

Group Membership
---
- Failure rate are norm in datacenters.

- Say, the rate of failure of one machine is 10 years (120 months). In a DC with
  120 servers, Mean Time To Failure (MTTF) is 1 month. MTTF reduces with more
  servers.

- Two Sub Protocols are essential in Group Membership.
    + Failure Detection
    + Info Dissemination

Failure Detectors
---
- Frequency of failures goes up linearly with size of DC.
- Properties of Failure Detectors
    + Completeness - each failure is detected.
    + Accuracy - no mistaken detection.
    + Speed - time to first detection of failure.
    + Scale - load balance each member.

- Completeness + Accuracy = Very hard to acheive. Consensus problem in
  distributed systems.

- What *really* happens?
    + Completeness - Gauranteed
    + Accuracy - partial/probabilistic gaurantee.


Failure Detector Protocols

Centralized Heartbeating.
---
- A process receives heartbeat from all other processes. After timeout, a
  process is marked failed.
- A process can be overloaded with heartbeats. Hotspot problemm.

Ring Heartbeating
---
- Form a ring. Processes send/receive heartbeats to its neighbors.
- Multiple simultaneous failures causes problems.

All-to-All Heartbeating
---
- All processes send to all other processes.
- Has equal load per member.
- Protocol is complete.
- Too many messages though.


Gossip-Style Membership
---
- Nodes maintain a membership list with 3 values:
    + IDs of Nodes.
    + Heartbeat counter. A counter unique to a node.
    + Local time when heartbeat was received from a node.

- Each node randomly shares this membership list with peers. Peers can update
  their membership list based on this message.

- There's a timeout if heartbeat is not received from a node, called T_fail.
- After a wait time of T_cleanup (mostly same as T_fail), the entry is deleted.

- If a heartbeat is received *directly* from the node then entry is added back
  to membership table.

- If membership tables are received from other nodes in T_fail time and before
  T_cleanup time, not updated.


Which is best failure detector?
---
- Failure Detector Properties:
    + Completeness - Guarantee always
    + Accuracy - Probability Mistake in T time units: PM(T)
    + Speed - T time units (time to first failure detection)
    + Scale - N x L (nodes x load per node)

- All-to-All heartbeating:
    + Each node sends N heartbeats every T time units. Load = N/T

- Gossip-Style:
    + tg = gossip period.
    + T = time when all nodes receive gossip.
    + T = (log N) x tg
    + L = Load per node = N/tg = (N x (log N))/T

- What's best/optimal?
    + TODO: Watch video 2.4 of Week 2 for analysis


SWIM Failure Detection Protocol
---
- TODO: Watch videos 2.5 & 2.6 of Week 2 for analysis.


Grid Computing
---
- Scheduling jobs on the Grid is one problem.
- Globus Protocol - inter-site job scheduling protocol
- HTCondor Protocol (High Through Condor) - intra-site job scheduling and
  monitoring protocol.
- Globus Alliance - bunch of univs and research labs.
- Globus Toolkit: Lot of tools (free and open-source) for Grid computing.

- Is Grid Computing and Cloud Computing converging? It's an open question.

P2P Systems Intro
---

Napster
---
+ Each user runs a client called Peers.
+ When a file is uploaded, it stays local.
+ Meta data about the file is uploaded to Servers (file name, IP, port number,
artists name etc).
+ Servers don't save files (hence no legal liability).
+ When someone searches for a file, Server responds with data.
+ Clients then establish connection and transfer files. Server is not involved.
+ Message exchanges using TCP sockets.
+ Every Server is root of a Ternary tree, with Peers as children.

How do Peers join Servers?
---
+ Send a http request to well-known URL like: www.myp2pservice.com

Problems
---
+ Centralized server a source of congestion, single point of failure, no
security.
+ Indirect infringment of copyright law.

Gnutella
---
+ There's no Servers. Clients themselves act as Servers to find and retrieve
data. It's called "Servants"
+ Peers stores files locally and have info about neighbor Peers.
+ This forms an Overlay Graph.
+ Gnutella protocol has 5 main message types:
    - Query (Search)
    - QueryHit (response to search)
    - Ping (to probe network for other peers)
    - Pong (reply to ping)
    - Push (used to initiate file transfer)

+ Gnutella Protocol Header
    +----------------------------------------------------+-------
    | Desc ID | Payload Descr | TTL | Hops | Payload len | ...
    +----------------------------------------------------+-------

    Desc ID: ID of this search
    Payload Descr: 0x00 Ping; 0x01 Pong; 0x40 Push; 0x80 Query;
                    0x81 QueryHit
    TTL: Initially 7-10, drop after reaching 0.
    Hops: Increment at each Hop. Used this because TTL is not same for all
        clients. Many clients don't use Hops that much.
    Payload len: # of bytes of message following this header.

+ Queries are flooded, TTL-restricted, duplicates are not forwarded.
+ QueryHit looks as follows:
    +--------------------------------------------------------------+
    |# of hits|port|IP addr|speed|(file idx,fname,fsize)|servent_id|
    +--------------------------------------------------------------+

    - servant_id is mostly not used.
    - Peers maintain reverse route, where they received Query msg from.
    - QueryHits are forwarded back on the same route.

Avoiding Excessive Traffic
---
- Query forwarded to all neighbors except one from which it received.
- Each Query forwarded only once.
- QueryHit routed back only to peer from which Query was received.
- If peer is missing, QueryHit is dropped.


Dealing with Firewalls
---
- Push message handles this (TODO: I didn't understand)

- Ping & Pong are used to update neighbor info.

Problems
---
- Ping/Pong constituted 50% of traffic (this has been solved).
- Cache and forward that info, reducing ping/pong messages.
- Problem of "freeloaders". 70% of users are freeloaders.
- Can the Query be directed instead of flooding?


FastTrack & BitTorrent
---
- FastTrack is proprietary. Hybrid between Gnutella and Napster.
- Uses "healthier" peers in Gnutella, called Super Nodes.
- Peers become Super Nodes based on reputation and contribution.
- Super Node maintains directory of files and related info. Queries are not
  flooded but searched locally.


BitTorrent
---
- Incentivize peers to participate.
- Tracker keeps track of peers. When a peer comes online, it contacts Tracker to
  send details of other peers.
- Peers are either Seed (contains full file) or Leech (partial file).
- Files are divided in to blocks (usually of equal size 32K-256KB).
- Leeches get blocks from Seed and other Leeches.

Which blocks are transferred to a Leech?
---
- Download Local Rarest First block policy
    + prefer early download of blocks that are least replicated among neighbors.

- Tit for tat bandwidth usage: Provide blocks to neighbors that provided it best
  download rates.
    + Incentive for nodes to provide good download rates.

- Choking: Limit number of neighbors to which concurrent uploads (less than 5).
  These are *best* neighbors.


Chord (came from Academia)
---
- Distributed Hash Table.
- Performance Goals of DHT
    + load balancing
    + fault-tolerant
    + efficiency of lookups and inserts
    + locality

- Napster, Gnutella, FastTrack are all sort of DHTs, but Chord is accurate DHT.
- Chord is O(log N) memory, lookup latency and # of messages for a lookup.
- Chord uses Consistent Hashing on nodes peer address
    + SHA-1 (ip address, port) --> 160 bit string.
    + Truncated to m bits
    + Called peer id (number between 0 and 2^m -1)
    + Not unique, but id conflicts very unlikely.
    + Can then map peers to one of 2^m logical points on a circle.

- Peer Pointers (1):
    + Peers are placed on a ring, with an id between 0 and 2^m-1
    + Peers talk to their neighbors, called successors.

- Peer Pointers (2): Finger Tables
    + TODO: Watch video 5 around 11th minute to understand.
    + ith entry in finger table for peer n is first peer with:
            id >= (n + 2^i) (mod 2^m)

- Where are files stored?
    + SHA-1 (filename) --> 160 bit string (key)
    + File is stored at first peer with id >= it's key (mod 2^m)
    + TODO: Very mathematical. Watch the video if required.

- How do file searches work?
    + Hash the unique file name (or URL) using consistent hashing algo.
    + It gives key K.
    + At node N, send query for key K to largest successor/finger entry <= K
      If none exists, send query to successor(N). Largest successor means, on
      the virtual ring, it's the right most, but less than K.




Pastry
---
- Assigns IDs to nodes, similar to Chord.
- Each node knows its successors and predecessors.
- Routing tables based on prefix matching, thus log(N)
- Among potential neighbors, one with shortest RTT is chosen.
- Shorter prefix neighbors are closer than longer prefix neighbors.
- But overall the longest neighbors RTT is comparable to underlying internet
  speed.





Key-Value Store Abstraction
---
- Similar to dictionary data struct, but distributed. Distributed hash as in
  P2P systems.
- In RDBMS (ex. MySQL), data is structured and stored in tables with few
  main items:
        Primary Key, Secondary Key, Foriegn Key and few columns.
- Data in today's world is different than problems solved by RDBMS
    + Data is large and unstructured
    + Lots of random read/writes (sometimes write heavy)
    + Foreign keys rarely needed
    + Joins are infrequent

- Requirements of Today's workload
    + Speed
    + Avoid single point of failure (SPoF)
    + Low TCO (Total Cost of Operation)
    + Scale out and not up

- Scale Out, Not Scale Up
    Scale Up = grow cluster capacity by replacing with more powerful m/c
    Scale Out = grow by adding COTS m/c (off the shelf)

Key-Value (NoSQL) Data Model
---
- NoSQL = "Not Only SQL"
- Main operations = get(key), put(key, value)
    + and few more extended operations
- Tables have following properties
    + "Column Families" in Cassandra, "Table" in HBase, "Collection" in
      MogoDB
    + Unstructured.
    + Don't always support JOIN or have Foreign Keys
    + Can have index tables like RDBMS




- Tables have following properties
    + "Column Families" in Cassandra, "Table" in HBase, "Collection" in
      MogoDB
    + Unstructured.
    + Don't always support JOIN or have Foreign Keys
    + Can have index tables like RDBMS


Column-Oriented Storage
---
- RDBMS store an entire row together
- NoSQL systems typically store a column together (or group of columns)
    + given a key, entries within a column are indexed are easy to locate
      and vice-versa.
- Searches involving columns are fast.
    + Ex: Get all blog id's that were updated in last month.
- Prevents fetching entire table (or row) on a search.


Apache Cassandra
---
- Facebook is original designer. Now managed by Apache and many companies
  use it.

Key->Server Mapping (called Partitioner)
---
- Uses Virtual Ring DHT described in Chord protocol/lecture.
    DHT = Distributed Hash Table

Data Placement Strategies
---
Two Options:
    + SimpleStrategy
    + NetworkTopologyStrategy

SimpleStrategy:
    + Uses Partitioner, of which there are two kids
        * RandomPartitioner - Chord like hash partitioning.
        * ByteOrderPartitioner - Assigns ranges of keys to servers.

NetworkTopologyStrategy: for multi-DC deployments
    + 2 replicas/DC, 3 replicas/DC
    + Per DC:
        * First replica placed according to Partitioner
        * Then go clockwise around ring until you hit a different rack.

Snitches:
- Maps IPs to Nodes in Racks and DCs. Configured in cassandra.yaml
- SimpleSnitch: Unaware of Topology (rack-unaware)
- RackInferring: Octect of IP indicates DC and Rack.
    + x.<DC>.<rack>.<node> - This is a best effort.
- PropertyFileSnitch: Uses a config file to assign IP
- EC2Snitch:
- Variety of snitch options available.

Writes - How are they implemented?
---
- Need to be lock-free and fast (no disk seeks)
- Clients sends write to a coordinator in Cassandra cluster.
- Coordinator uses Partitioner to send query to all replica nodes responsible
  for key.
- When X replicas respond, coordinator returns an ACK to client.
    + What is X?
- Replicas must be ALWAYS writable. How to achieve?
    + Hinted Handoff Mechanism
        * If replica is down, coordinator writes to all other replicas, but
          waits for replica to come up within a time limit.
        * If ALL replicas are down, coordinator waits for a time (few hours) and
          then writes to replica.
    + One ring per DC.
        * Per DC coordinator elected to talk to other DCs.
        * Election of coordinator done via Zookeeper.

When writes come, what does Replica do?
---
- log the request.
- Make changes to Memtable (in-memory representation of key-value pairs)
- memtable can be searched. It's a write-back cache as opposed to write-through
  cache.
- When memtable is full, flush to disk.
- Before flushing to disk, sort keys of Memtable in to SSTable (Sorted String
  Table). This is a Data File.
- Create an Index file: An SSTable of (key, position in data SSTable).
- Add a Bloom Filter to tell if a key is present in SSTable. Very efficient.
    + An item NOT in present can return as present.
    + An item present ALWAYS returns as present.

Compaction
---
- Over time, key may be in multiple SSTable and compaction is process of merging
  them. Runs periodically and locally at each server.

Deletes
---
- Don't delete item rightaway, but put a marker called "tombstone". When
  Compaction is run, it deletes.


Reads
---
- Coordinator can contact X replicas. From responses, Coordinator returns latest
  time-stamped response.
- Coordinator also fetches value from other replicas (in background) to check
  consistency and initiating *read repaid*.
- Due to this, Reads are slower than Writes.

Membership
---
- Every server in cluster has list of all others in the cluster.


Suspicion Mechanisms
---
- Mechanism to confirm failure of a server is indeed true (or false).
- Accrual Detector: it outputs a value called Phi, which represents suspicion.
- Phi calculations for a member:
    + takes in to account inter-arrival times for gossip messages from a server.
    + Phi(t) = -log(CDF)/log10
        * CDF = Cumulative Distribution Function or Prob(t_now - t_last)
    + Phi determines the detection timeout by taking in to historical inter
      arrival time.

- In practice, Phi=5 means about 10-15s detection time.

Cassandra Vs RDBMS (MySQL): On 50 GB of data
---
- MySQL:
    + writes 300ms avg; reads 350ms avg
- Cassandra:
    + writes .12ms avg and reads 15ms avg



CAP Theorem
---
- Consistency, Availability, Partition-Tolerance
- Only 2 out of 3 can be satisfied.
- Cassandra: Eventual (Weak) Consistency, Availability & Paritition-Tolerance.
- RDBMs: Strong Consistency over Availability.

- RDBMS provide ACID
    + Atomicity
    + Consistency
    + Isolation
    + Durability

- Key-Value Stores like Cassandra provide BASE
    + Basically Available Soft-state Eventual Consistency

- What is X in Cassandra?
    + Represents consistency levels
- X = ANY
    + Any server can respond to read. Fastest.

- X = ALL
    + All replicas must respond. Slowest.

- X = ONE
    + At least one replica must respond. Faster than ALL. Can't tolerate
      failure.

- X = QUORUM
    + Look at Vide (1.3) around 13th minute.
    + Faster than ALL. Gives greater consistency than ONE.
    + Many NoSQL uses Quorum (Cassandra, RIAK)

Reads with Quorum
---
- Client specifies R (# of replicas).
- Read must be received from R replicas.

Write with Quorum
---
- Client specifies W and writes to W replicas and returns.
- Coordinator (a) blocks until quorum is reached OR (b) Asynchronous; two
  flavors exist.

- For strong consistency two conditions required:
    1) W+R > N
    2) W > N/2
    R = read replica count; W = write replica count


QUORUM Variations
---
- QUORUM: across all DCs
- LOCAL_QUORUM: in coordinators DC
- EACH_QUORUM: in every DC

Consistency Spectrum
---
- From Weak to Strong
- Eventual, Causal, per-key Sequential, Red-Blue, Probabilistic, CRDTs,
  Sequential.
- Per-Key Sequential: Per-Key basis, all operations have a global order.
- CRDTs (Commutative Replicated Data Types):
    + Operations for which commutated writes give same result.
    + Ordering is unimportant in that case.
- Red-Blue: Divide operations in to Red and Blue. Order is MUST/important for
  Red operations but not so for Blue.
- Causal: Reads and Write are causally linked. One happens before other or
  depends on other.

What are models of Strong Consistency?
---
- Linearizability: Each op is visible instantaneously to all other clients.
- Sequential Consistency [Leslie Lamport]:
- Some NoSQL are supporting ACID.


HBase
---
- Yahoo implemented Google's BigTable called HBase.
- API Functions:
    + get/put(row)
    + scan(row range, filter) - range queries
    + multiput
- HBase prefers consistency over availability.


HBase Architecture
---
- Lookup online.
- HDFS is underlying storage.


HBase Storage Hierarchy
---
- HBase Table
    + Split in to multiple regions and replicated.
    + ColumnFamily = subset of columns with similar query patterns.
    + One "Store" per combination of ColumnFamily and Region.
        * "Memstore" for each Store: in-memory updates to Store. Flushed to disk
          when full.
        * StoreFiles for each Store (HFile is underlying format)

- HFile
    + SSTable from Google's BigTable

How does HBase maintain Strong Consistency?
---
- Using Write-Ahead Log, called HLog.
- HLog is written so if there's a failure in Memstore, HRegionServer/HMaster can
  replay the message from HLog and write to Memstore.



Cross-Datacenter Replication
---
- Single "Master" cluster.
- Other "Slave" clusters replicate same tables.
- Master cluster synchronously sends HLogs over to Slave clusters.
- Coordination among clusters via Zookeeper.
- Zookeeper can be used as a file system to store control info.



Time and Ordering
---
Asynchronous Distributed Systems:
---
- Different clocks and diff systems all together.

Clock Skew vs Clock Drift
---
- CS: Relative diff in clock values of two processes
    + Like distance between two vehicles on a road.
- CD: Relative diff in clock frequencies (rates) of two processes.
    + Like difference in speeds of two vehicles on the road.

- non-zero CS means, clocks are not synchronized.
- non-zero CD causes CS to increase.


How often to synchronize?
---
- MDR: Maximum Drift Rate of a clock
- Coordinated Universal Time (UTC): is "correct" time at any point.
- Absolute MDR is defined relative to UTC.
- Max drift rate between two clocks with similar MDR = 2*MDR
- Given a max acceptable skew M, between any pair of clocks, need to synchronize
  at least once every: M/(2*MDR)
    + Since time = distance/speed


External vs Internal Synchronization
---
External Sync
---
- Each process clock C is within a bound D of a well-known external clock S.
    |C-S| < D at all times.
- Ex: Cristian's algorithm and NTP

Internal Sync
---
- Every pair of processes in a group have clocks within bound D
    |C(i) - C(j)| < D at all times for processes i and j.
- Ex: Berkeley algo.

- External Sync with D = Internal Sync with 2*D
- Internal Sync does not imply External Sync
    + In fact, the entire system may drift away from external clock S


Cristian's Algorithm
---
- External time sync. All processes P sync with external time server S.
- P measures RTT of message exchanges.
- min1 = minimum P->S latency
- min2 = minimum S->P latency
- Actual time at P when it receives response is between:
    [t+min2, t+RTT-min1]
        Note that RTT > (min1+min2)
- P sets its time to halfway through this interval
    t+(RTT+min2-min1)/2
- Erro is at most (RTT-min2-min1)/2
    + Bounded error.

Gotchas
---
- Allowed to increase clock value but not decrease, to maintain linearity of
  events. Otherwise it may violate ordering of events within a process.
- Allowed to increase or decrease clock speed.
- If error is too high, take multiple readings and average them.


NTP
---
- NTP servers organized in a tree.
- Root of tree = Primary Servers, where UTC time/clock present.
- Children of Root = Secondary Servers.
- Grandchildren of Root = Tertiary Servers.
- Each client is leaf of the tree.


                 TR1     TS2
Child ------------^---------------------^-------------------------->
        \        /         \           / Parent sends
         \      /M1         \ M2      / (TS1, TR2)
          \    /             \       /
Parent ----v------------------v------------------------------------>
           TS1               TR2

- Child uses all these times to calculate it's clock.

- Offset O = (TR1-TR2+TS2-TS1)/2

- Suppose real offset between Child and Parent is oreal
    + Child is ahead of Parent by oreal
    + Parent is ahead of Child by -oreal

- Suppose one-way latency of M1 is L1 and of M2 is L2. L1 and L2 are unknown.
- TR1 = TS1 + L1 + oreal
- TR2 = TS2 + L2 - oreal
- Subtracting both equations to get oreal

    oreal = (TR1-TR2+TS2=TS1)/2 + (L2-L1)/2
    oreal = O + (L2-L1)/2
    |oreal-O| < |(L2-L1)/2| < |(L2+L1)/2|
        + Thus, the error is bounded by RTT.

- There's always non-zero error. Can't we *not* synchronize clocks? Directly
  address the issue of events ordering?


Lamport (Logical) Timestamps
---
- Assign timestamps to events that were not *absolute* time, but obey
  *causaility*.

- Define a logical relation Happens-Before among pairs of events.
- Happens-Before denoted as ->

Three Rules
    1) On the same process: a -> b, if time(a) < time(b), using local clock.
    2) If p1 sends m to p2: send(m) -> receive(m)
    3) If a -> b and b-> c then a -> c (Transitive Property)
        + Not all events related to each other via ->

- These rules create partial order among events. Two processes that never
  communicate have their events that are concurrent.


Implementation of Lamport Timestamps
---
Goal: Assign logical timestamps to each event.
- Timestamps obey causality.
- Rules:
    + Each process uses a local counter, an int, initialized to 0.
    + Process increments its counter when a *send* or an *instruction* happens
      at it. Counter is assigned to the event.
    + A *send* message event carries its timestamp
     + For receive message event, the counter is updated by:
        receive timestamp = max(local clock, message timestamp) + 1

Concurrent Events
---
- A pair of concurrent events doesn't have causal path from one event to
  another.
- Lamport timestamps not guaranteed to be ordered or unequal for concurrent
  events.
- E1 -> E2 means timestamp(E1) < timestamp(E2), However, opposite is not true:
    if timestamp(E1) < timestamp(E2) then E1 -> E2 OR E1 and E2 are concurrent.
- Concurrent events can't be identified.

Vector Clocks
---
- Used in Key-Value store like Riak
- Each process uses a vector of integer clocks.
- Supposed there are N processes in the group then each vector has N elements.
- Process i maintains vector Vi[1...N]
- jth element of vector clock at process i, Vi[i], is i's knowledge of latest
  events at process j.
- Each msg carries senders vector
- On receiving a message at process i:
        Vi[i] = Vi[i] + 1
        Vi[j] = max(Vmsg[j], Vi[j]) for i not equal j

- Obey's causality. Watch video 2.5 at 5min 40s.
- More space consuming, but this idea can capture concurrent events.


Global Snapshot
---
- In a distributed system, how do you calculate a "global snapshot"? What
  does it even mean and uses?
    + Checkpointing: can restart applications on failure.
    + Garbage Collection: of unused objects.
    + Deadlock Detection: useful in DB transactions.
    + Termination of Computation: useful in batch computing.

- Global Sanpshot = Global State = Individual state of each process +
  Individual state of each communication chaneel. Channels are FIFO ordered.

- Capture *instantaneous* state of each process.
- Capture *instantaneous* state of each communication channel.

First Solution:
---
- Sync all processes and record states at time t.
- Probs:
    + Time sync always has errors.
    + Doesn't record state of msgs in channels.
- However, sync is not required. Causality is enough.


                Cji
       <---------------------
    Pi                       Pj
       --------------------->
                Cij


- Next solutions obey causality.

Chandy-Lamport Algorithm of Global Snapshot (classical algo)
---
Problem: Record a global snapshot for each process state and comm channel.
System Model:
- N processes in the system.
- Two uni-directional comm channel between processes Pi and Pj
    + FIFO ordered

Assumptions:
- No failure on channel
- All messages arrive intact and no duplicates.

Requirements:
- Snapshot shouldn't interfere with normal apps.
- Each process is able to record it's own state.
- Global state is collected in distributed manner.
- Any process may initiate a snapshot.

Snapshot Initiator Process Pi
---
- Pi records its own state first.
- Pi creates special messages called "Marker" messages.

for j=1 to N, except i
    Pi sends out a Marker msg on outgoing channel Cij
    // thats N-1 channels
- Starts recording incoming messages on each of incoming channels at Pi:
    Cji for j=1 to N except i

When Pi receives a Marker message on Cki
---
- If this is 1st Marker Pi is seeing
    + Pi records it's own state first.
    + Marks the state of channel Cki as "empty"
    + for j=1 to N except i
        * Pi sends out a Marker msg on outgoing channel Cij
    + Starts recording the incoming messages on each of incoming channels at
      Pi: Cji for j=1 to N except i and k
- Else // already seen a Marker msg
    + Mark all messages received since first Marker on Cki as part of the
      state of Cki and stop recording state on that channel.

- Note that state of a channel is Marked by the receiving end of the
  channel.

- Algo Termination:
    + All processes receives a Marker msg.
    + All processes have received a Marker on all N-1 incoming channels

- This algo is Causally Correct.


Consistent Cuts
---
Cut = Time point at each process and at each channel.
- Events at the process/channel that happen before the cut are "in the cut"
  and after the cut are "out of the cut".

Consistent Cut: a cut that obeys causality
- A cut C is consistent iff for each pair of events e and f such that event
  e is in the cut C and if f->e (f happens-before e) then f is in cut C.


- In Chandy-Lamport Global Snapshot algo, the Consisten Cut is the line
  through all the starting points of each process recording time.


Safety & Liveness Properties
---
- These properties determine "Correctness" in distributed systems.

- Liveness = guarantee that something "good" will happen "eventually".
    Ex:
    + Distributed computation will terminate eventually.
    + Every failure is eventually detected by some non-faulty process.
    + In Consensus, all processes eventually decide on a value.

- Safety = guarantee that something "bad" will "never" happen.
    Ex:
    + There's no deadlock in a distributed transaction system.
    + No object is orphaned in a distributed object system.
    + "Accuracy" in failure detectors.
    + In Consensus, no two processes decide on different values.

- Very hard to guarantee both Liveness and Safety in Async Dist systems.
    Failure Detector: Completeness (Liveness) and Accuracy (Safety) cannot
                      both be guaranteed.
    Consensus: Decisions (Liveness) and Correct Decisions (Safety) cannot
               both be guaranteed.
    Legal: Very hard for legal systems to guarantee that all criminals will
           be jailed (Liveness) and no innocent is ever jailed (Safety).


What does Liveness and Safety mean in Global Snapshot/State world?
---
- Dist sys moves from one state to another.
- Liveness, w.r.t a property Pr in a given state S means
    + S satisfies Pr or there is some causal path from S to S' where S'
      satisfies Pr.
- Safety w.r.t a property Pr in a given state S means
    + S satisfies Pr and all global states S' reachable from S also satisfy
      Pr.

- Stable Properties = once true, stays true forever.
- Chandy-Lamport algo can be used to detect global stable properties.

- Stable Liveness example: Computation terminated.
- Stable non-Safety example: There's deadlock.

- Due to its Causal correctness, all stable properties can be detected using
  Chandy-Lamport algo.


Multicast Ordering
---
- A msg sent to multiple processes in a group.

Who uses mutlicast?
---
- Widely used abstraction in all cloud systems.
- Replica servers for a key multicasts read/writes within the replica group.
- Membership info is multicast across all servers in a cluster.

- Types of ordering in Multicast
    + FIFO
    + Causal
    + Total Ordering

- Causal ordering implies FIFO ordering. Reverse is NOT true.
- Causal ordering is intuitive and preferred.
- Total ordering is aka "Atomic Broadcast".
- Unlike FIFO and Causal, Total Ordering does not pay attention to order of
  multicast sending.
- Total ordering ensures all receivers receive all multicasts in the same
  order.


FIFO Multicast: Data Structs
---
- Each receiver maintains a per-sender sequence numbers
    + Process P1 through Pn
    + Pi maintains a vector of seq. no. Pi [1...n], zero initialized.
    + Pi[j] is latest seq. no. Pi has received from Pj.

- Sender Pj:
    + Pj[j]++
    + Include new Pj[j] in multicast msg.

- Receive multicast: If Pi receives a multicast from Pj with seq. no S in
  msg then:
    + if (S == Pi[j] + 1) then
        * deliver msg to application
        * set Pi[j]++
    + else buffer this multicast until above condition is true.


Ex: Please see lecture 2.2 starting at 4.00 minutes

Total Ordering: Ensures ordering of messages are same across all receivers.
---
- Sequence based approach.
- Special process elected as leader or sequencer.

- Sender Pi sends multicast msg to group and sequencer.

- Sequencer:
    + Maintains a global sequence number S (initially 0)
    + When it receives a multicast msg M, it increments S and multicasts M
      and S.

- Receive multicast at Pi:
    + Pi maintains a local received global seq. no. Si (initially 0).
    + If Pi receives a multicast M from Pj, it buffers it until following
      two conditions are satisfied:
        * Pi receives <M, S(M)> from Sequencer AND
        * Si + 1 = S(M)
        * Then deliver its msg to application and set Si=Si+1


Causal Ordering Impl: Data Struct
---
- Each receiver maintains a vector of per-sender seq. no.
- Similar to FIFO multicast, but updating rules are diff.
- Processes P1 to Pn
- Pi maintains a vector Pi[1...n] (initially 0)
- Pi[j] is the latest seq. no Pi has received from Pj





@@@@@@@@@@@@@@@@@@@@@@@   C++ for C Programmers    @@@@@@@@@@@@@@@@@@@@@@@@

- static_cast <new_type> (expression), dynamic_cast <new_type> (expression)
  const_cast <new_type> (exp), reinterpret_cast <new_type> (exp)

C++ is better than C (really?)
---
    + More type safe
    + More libraries
    + Less reliance on preprocessor
    + OO vs imperative

C++ Templates
---
    template <class T>
    inline void swap (T& d, T& s)
    {   T temp = s;
        s = d;
        d = temp;
    }

C++ Generics: Sum an array
---

template <class T>  //T is a generic type
T sum (const T data[], int size, T s=0)
{
    for (int i=0; i<size; i++)
        s += data[i]; // += must work for T

    return s;
}

- Defaults for function parameter in C++ must be at the end of the param list.
- When multiple default params exists, in a function call, values are assigned
  left to right.
        void func(int x=10, int y=20, int z=30);
        func(1, 2, 3);  // x=1, y=2, z=3
        func(1, 2);     // x=1, y=2, z=30
        func(1);        // x=1, y=20, z=30
        func();         // x=10, y=20, z=30


Multiple Template Arguments
---
- Be careful, as it can get very dangerous.

template <class T1, class T2>
void copy (const T1 src[], T2 dst[], int size)
{
    for (int i=0; i<size; i++) {
        dst[i] = static_cast<T2> (src[i]);
    }
}

- static_cast<> This is safe casting.

C++ casts
---
- 4 types of casting in C++
    + static_cast <type>    // considered safe
    + dynamic_cast <type>   // used with classes
    + reinterpret_cast <type>   // highly unsafe, similar to C
    + const_cast <type>     // cast away const-ness

    reinterpret_cast and const_cast are usually discouraged by instructor.

Representation of Graph
---
- Edge list representation
- Connectivity matrix (also distances).


List representation:
---
- Representation of directed graph with n verticies using an array of n lists of
  vertices.
- List i contains vertex of j if there is an edge from vertex i to vertex j.
- A weighted graph may be represented with a list of vertex/weight pairs.
- An undirected graph may be represented by having vertex j in the list for
  vertex i and vertex i in the list for vertex j.

Matrix Representation:
---
- Well known.

- Both these don't capture weight.


Dijkstra Shortest Path
---
TODO: Watch Prof. Bob Sedgwick's video.


Enum
---
typedef enum color {
    RED,
    WHITE,
    GREEN,
} color;

- enum is of type int.
- Unary operators can be overloaded. Their precedence cannot be overriden.


Operator Overloading
---

- Example of overloading ++ operator

typedef enum days {
    SUN,
    MON,
    TUE,
    WED,
    THU,
    FRI,
    SAT
} days;

inline days operator++ (days d)
{
    return static_cast <days> ((static_cast<int>(d)+1)%7);
}

- Example of overloading << operator

ostream& operator<< (ostream& out, const days& d)
{
    switch (d) {
        case SUN: out << "SUN"; break;
        case MON: out << "MON"; break;
        ...
    }

    return out;
}

- Combine above two examples

int main()
{
    days d = MON, e;
    e = ++d;    // this is calling operator++ (days) function
    cout << d << '\t' << e << endl; // calling operator<< (out, days)
}


C++ Classes
---
- Using example of a point in 2D graphs

class Point {
    public:
        double getx () {return x;}
        void setx (double v) {x=v;}

    private:
        double x,y;

}

- public, private (default), protected are access levels.

point operator+ (point &p1, point& p2)
{
    point sum = {p1.x+p2.x, p1.y+p2.y};
    return sum;
}

ostream& operator<< (ostream& out, const point &p)
{
    out << "(" << p.x << ", " << p.y << ")";
    return out;
}

- constructors
    + default constructor has no args.
    + Same name as class name.

- 'this' keyword is self referential pointer.
- destructor

- Constructor Example:

class point {
public:
    point (double x=0.0, double y=0.0):x(x), y(y) {} // constructor
    ...
private:
    double x,y;
}

    - Note the initializer list x(x), y(y). This is equivalent to
        this->x = x, this->y = y;
    - Can be done only in constructors.

Memory Management
---
- new --> equivalent to malloc()
- delete --> equivalent to free()
- They both are allocated from heap. Unlike Java, memory from heap are not
  garbage collected automatically.

Ex:
    char *s = new char[sz];
    int *p = new int(9); // single int, initialized to 9
    delete []s; // delete an array
    delete p;   // delete single element

~ destructor
---
- Destructor always has empty arg list.
- Can't be overloaded.

Randomly Generated Graph
---
    bool** graph;
    srand(time(0)); // seed
    graph = new bool*[size];
    for (int i=0; i<size; i++)
        graph[i] = new bool[size];

//2D array representing a graph

Density = Probability an edge exists, between 0 and 1

- Suppose Density=0.19

    for (int i=0; i<size; i++)
        for (int j=0; j<size; j++) {
            if (i==j) graph[i][j] = false;
            else graph[i][j] = graph[j][i] = (prob()<density);
        }

is_connected algorithm
---
    bool is_connected (bool* graph[], int size)
    {
        int old_size=0, c_size=0;
        bool* close = new bool[size];
        bool* open = new bool[size];
        for (int i=0; i<size; i++)
            open[i] = close[i] = false;
        open[0] = true;
        ...
    }

- Each iteration, add one node to closed set.

- Copy Constructors, Deep Copy vs Shallow Copy (or Referential Copy).

[1] Coursera course by Ira Pohl (Aug 2016)

@@@@@@@@@@@@@@@@@@@@@@@   OpenSwitch Tutorials     @@@@@@@@@@@@@@@@@@@@@@@@

OVSDB Overview and Troubleshooting
---
- Pub/Sub model
- Daemons connect to OVSDB server on unix socket and register for set of
  tables/columns they are interested.
- Daemons get notifications on change to these tables/columns.
- Notifications have old and new values.
- Communication happens over JSON-rpc and follows OVSDB RFC.

- vswitch.extschema - contains info on all tables/columns. OVSDB server looks
  for this on start.
- vswitch.xml - explanation of each table/column.

Commands
--
$ ps -ef | grep ovsdb-server


@@@@@@@@@@     C++ @@@@@@@@@@@
Readup More On:
- References (vs Pointers)
- try/catch (Exception Handling)

Notes:
- Access global variables in local scope using ::

    using namespace std;
    #include <iostream>

    double a = 128;

    int main ()
    {
       double a = 256;

       cout << "Local a:  " << a   << endl;
       cout << "Global a: " << ::a << endl;

       return 0;
    }

- Use 'template' to accept any datatype in to a function

    template <class T, class U>
    T minimum (T a, U b)
    {
       T r;

       r = a;
       if ((T) b < a) r = (T) b;	// typecasting b to T

       return r;
    }

- new, delete, new[], delete[]; allocate and free memory.
	char *c = new char [15];
	delete[] c;

-

@@@@@@@@@@@    STANDARD   TEMPLATE   LIBRARY    @@@@@@@@@@@@@

- STL containts five kinds of components:
    * Containers: vectors, bit vectors, lists, deques, sets, multisets, maps,
                  multimaps, queues, stacks and priority queues.
    * Iterators
    * Algorithms
    * Function Objects
    * Allocators


Vectors
=======
#include <vector>

using namespace std;

vector<T> N;    // Empty vector
vector<T> N[10]; // An array of 10 vectors. "May be" not what we want.
vector<T> N(10); // Vector of size 10.
int count = v.size();   // size of vector. Don't use this to determine if vector
                        // is empty.
bool is_empty = v.empty();

vector<int> v;
v.push_back(100);   // adds 100 at end of vector. Don't worry about memory
                    // allocation. It's NOT done one at a time.

v.resize();     // resize

NOTE: Is you push_back() after resize(), new elements are added AFTER newly
created memory and NOT INTO it.

v.clear();  // vector now has zero elems.


Vector Initialization
-------
vector<int> v1;     // Default values are zeros
...
vector<int> v2 = v1;
vector<int> v3(v1);


vector<int> nums (20, 5);   // Default values are 5.
vector<string> names (20, "Unknowns");  // Defaults to "Unknowns"


Multidimensional Vectors
--------
vector< vector<int> > Matrix;
vector< vector<int> > Matrix(N, vector<int>(M));    // of size NxM, with zero
                                                    // default values
vector< vector<int> > Matrix(N, vector<int>(M, -1));    // of size NxM, -1
                                                        // initial value.



Pairs
=====

A simple form:

template<typename T1, typename T2> struct pair {
     T1 first;
     T2 second;
};

pair<int, int> P;   // pair of ints
pair<string, pair<int, int> > P;    // pair of string and two ints.

 pair<string, pair<int,int> > P;
 string s = P.first; // extract string
 int x = P.second.first; // extract first int
 int y = P.second.second; // extract second int



Iterators
=========

References:
[1]
https://www.topcoder.com/community/data-science/data-science-tutorials/power-up-c-with-the-standard-template-library-part-1/
[2]
http://cs.brown.edu/~jak/proglang/cpp/stltut/tut.html
[3]
http://www.tutorialspoint.com/cplusplus/cpp_stl_tutorial.htm
[4]
http://en.cppreference.com/w/cpp

@@@@@@@@@@@@@@@@    NAND 2 TETRIS COURSE    @@@@@@@@@@@@@@@@@@@@@@


Week 1: Boolean Logic
-----
Commutative Law:
---
x AND y = y AND x
x OR y = y OR x

Associative Law:
---
x AND (y AND z) = (x AND y) AND z
x OR (y OR z) = (x OR y) OR z

Distributive Law:
---
x AND (y OR z) = (x AND y) OR (x AND z)
x OR (y AND z) = (x OR y) AND (x OR z)


De Morgan Law:
---
NOT (x OR y) = NOT (x) AND NOT (y)
NOT (x AND y) = NOT (x) OR NOT (y)

Idempotent Law:
---
w AND w = w
w OR w = w



Truth Table to Boolean Expression: How to?
---
- For every row resulting in 1, write a boolean expression for that row alone
  (ignore other rows). OR all these expressions.

Theorem 1:
Any boolean function can be represented using an expression containing AND, OR
and NOT.

Theorem 2:
A boolean function can be represented using AND and NOT gates only.

Theorem 3:
A boolean function can be represented using NAND gate alone.

x NAND y = NOT (x AND y)

- How can an AND be done using NAND?
    x AND y = NOT (x NAND y)

- How can a NOT be done using NAND?
    NOT (x) = x NAND x

Logic Gates
---

Truth Table of NAND gate:
    x   y   out
    0   0   1
    0   1   1
    1   0   1
    1   1   0

Functional Spec: if (x==1 and y==1) then out = 0
                 else out = 1


Other elementary gates: AND, OR and NOT


Composite Gates:
---
- Some of logic gates to form a more complex gate.


HDL:
---
- HDL is a functional/declarative language.
- Order of HDL statements is insignificant.
- Interface of logic gates:
    Not (a=, out=), And (a=, b=, out=), Or (a=, b=, out=)

- Common HDLs: VHDL, Verilog and many more.
-


Multiplexor:
-----
Three Inputs: a, b, sel
One Output: out

if (sel ==0) out = a
else out = b


Demultiplexor: Inverse of Multiplexor
---
Input: in, sel
Output: a, b

if (sel==0) {a,b} = {in,0}
else {a,b} = {0,in}



Project 1: 3 types of logic gates will be built.

    Elementary Logic Gates: Not, And, Or, Xor, Mux, DMux
    16-bit variants: Not16, And16, Or16, Mux16
    Multi-way variants: Or8Way, Mux4Way16, Mux8Way16, DMux4Way, DMux8Way


















Reference:
https://www.coursera.org/learn/build-a-computer/
